<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MG ML – b-project-description</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">MG ML</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mgallimore88"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:mgallimore88@gmail.com"><i class="bi bi-envelope" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/michael-gallimore-4104536a/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">



<section id="cnn-birdsong-classifier-description" class="level1">
<h1>CNN Birdsong Classifier Description</h1>
<p>Process description for the <a href="https://github.com/Mgallimore88/osfl_cnn_recognizer">Olive Sided Flycatcher recognizer</a>.</p>
<p>This project is open sourced and available <a href="https://github.com/Mgallimore88/osfl_cnn_recognizer">here</a>, along with a more in depth model development walkthrough for people wanting to repeat the training process.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/osfl-dataset-creation-pipeline.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">dataset creation pipeline</figcaption><p></p>
</figure>
</div>
<p><em>Image - dataset and model development pipeline, described below</em></p>
<p><strong>Project brief</strong> - Build a model to detect the Olive Sided Flycatcher in audio recordings. - The model will be used on field recordings on audio collected on ARUs in BC, Canada. - Try to make the model as good as possible, try to beat the current state of the art.</p>
<section id="approach" class="level2">
<h2 class="anchored" data-anchor-id="approach">Approach</h2>
<p><strong>Train, test, inference similarity</strong></p>
<p>In order to train a model which would be useful during production, I decided to focus on creating a high quality training set which is as similar as possible to the data the model would encounter at inference time. I took measures to avoid overfitting which can happen when the model is evaluated on examples which it was trained on, leading to over-optimistic results.</p>
<p>The model needs to work on future audio recordings as well as a backlog of historic recordings. Therefore the training set should contain audio from a variety of recording qualities and devices; if the model only encountered high quality recordings during training, it may not perform well on low quality mp3 recordings at inference time.</p>
<p>I also learned that the model would need to be run on audio data collected in BC. There wasn’t a large quantity of data available in BC for training, however I had access to data collected in Canada from some nearby provinces, and others further away but which would use similar recording techniques to those used in BC. Access to field recordings from the USA would have been ideal, but I was unable to obtain these.</p>
<p>To simplify dataset creation, only the song of the Olive Sided Flycatcher was used, and not its call. The species call is less distinctive than its song. This was omitted from the positive labels unless it also occurred with the song.</p>
<p>At inference time, the model would be run on long audio recordings. A moving window is used to split the audio into segments, and the model is run on each of these segments. Since I was aiming to match the training dataset as closely as possible with the inference data, I used the same moving window technique to generate the training examples from source audio.</p>
<p>I trained a model exclusively on audio collected from ARUs in the field, and built an initial dataset of training examples entirely from within the habitat of the species, with the speices song either present or absent. This ensured that the model was really learning the shape of the bird song, and not the accompanying environmental sounds or any potential artifacts introduced by layering audio samples - this was a concern I had, but not one I had read about in literature.</p>
<p>Initially there was no set of labels for the absent class. By looking only at audio which contained a positive ID of the target species, I was able to infer an absent class by sampling from sections of audio before the first instance of the target species song. This was possible due to the tagging method used on WildTrax, where only the first vocalization of an individual is tagged.</p>
<p>I used a model to pull out additional close range songs from previously unlabelled portions of the recordings.</p>
<p>After that I added recordings from other locations outside of the species habitat. I paid particular attention to those model detections which were most confidently wrong, and added these to later training sets. In this way, the model was able to bootstrap examples into the training set which would contain edge cases and examples which had been most confusing (misidentified as the target species) during prior runs. This was a technique I adapted from the fastai course on deep learning, from a <a href="https://github.com/fastai/fastbook/blob/master/02_production.ipynb">chapter on image classification</a>, and have since learned has the name <strong>Hard Example Mining</strong>.</p>
<section id="data-source" class="level3">
<h3 class="anchored" data-anchor-id="data-source">Data source</h3>
<p>I was given access to recordings collected across Canada by various organizations, including the start and end times of olive sided flycatcher songs within these recordings, and the unique location IDs for each recording. As a proxy for the species habitat, I chose any location ID which had at least one Olive Sided Flycatcher song detected in it.</p>
</section>
</section>
</section>
<section id="model-training." class="level1">
<h1>Model Training.</h1>
<p>A step by step walkthrough for model training can be found in the repository for this project. This is a summary of that process.</p>
<section id="data-cleaning" class="level3">
<h3 class="anchored" data-anchor-id="data-cleaning">1. Data cleaning</h3>
<p>To begin with, I found all the target species songs labelled in WildTrax, and downloaded the recordings containing these songs. I then calculated the start and end times of a moving window along these recordings, and created labels indicating whether the target species was present or absent in each window, up until the first detection of the species in each recording.</p>
<p>After verifying the values in the training set, I found that out of the positive examples, 222 out of 3000 were mis labelled as containing the target species when they did not, and that in the inferred negative sections of audio (the section of audio before the first vocalization of the target) - 231/1806</p>
<p>I estimated the accuracy of the human labelled data I had to be around 70% - some species were mis-labelled, and sometimes there would be a label indicating the first detection of the target species when in fact there were earlier vocalizations. To train a decent model I would need to verify the labels manually.</p>
<p>Here are some of the cleaning steps taken at the level of the original database of tags: - remove any tags deemed ‘too many to tag’ - remove any tags not labelled as ‘song’ - remove any recordings not tagged in wildtrax - remove any tags which don’t contain a url link to audio - remove any duplicates or synthetic audio from the database</p>
<p>Available recordings were split into two sections - recordings from locations which had human labelled tags of the target species, and recordings from other locations which had never had the target species tagged there. The recordings from other habitats were put to one side until later stages of dataset development.</p>
</section>
<section id="train---validation---test-split" class="level3">
<h3 class="anchored" data-anchor-id="train---validation---test-split">Train - Validation - Test split</h3>
<p>Within those recordings from locations which contianed the target species, those same location IDs were used to further split the data into <strong>training</strong>, <strong>validation</strong> and <strong>test</strong> sets. The training data is what was used to train the model, and the validation data was used to evaluate the model throughout the training process - in order to get an idea of how the model performs on unseen data. The test data was withheld until the very end of model training, and was used to evaluate the model’s performance on a new set of completely unseen data, also from new locations. This is done to avoid overfitting to the validation set- which can happen if the apparent improvements to the model are actually just a lucky set of hyperparameters which work well for that particular set of validation data.</p>
<p>3 second window start and end times with a 1.5 second overlap were generated using OpenSoundScape’s AudioSplittingDataset class for all of the data, and labels were created using the following rules: - If the species detection tag falls fully within a window, the window gets the label 1 meaning present. - If the audio is from before the first detection of the species in the recording, the window gets the label 0 meaning absent. - If the detection tag partially overlaps the start or end of the window, the window is discarded.</p>
<p>This created an initial dataset of 3990 human labelled present examples, and 44,948 inferred absent examples available for training. The validation set was made up from an additional 1011 present and 10,596 inferred absent examples. The test set contained an additional set of examples of similar size to the validation set, and was set to one side until the end of the training process.</p>
</section>
<section id="train-an-initial-model" class="level3">
<h3 class="anchored" data-anchor-id="train-an-initial-model">Train an initial model</h3>
<ul>
<li>I used OpenSoundScape to set up the model training pipeline, handling image augmentation, conversion of the audio in realtime to spectrograms, batching of the training examples, SGD etc. This allowed me to focus more time on data cleaning and model evaluation.</li>
</ul>
<p>The model architecture chosen was a ResNet34, which is a convolution neural network pretrained on the imagenet dataset for image classification tasks. Transfer learning applied to this model allows us to take advantage of the training the model has already undergone on the imagenet dataset, and apply it to our own audio classification task. Further improvements could be made by choosing a model trained on spectrograms, or using another architecture such as EfficientNet, but the ResNet34 model seemed to be performing well at this task.</p>
<p>A sample of 1000 present and 1000 absent samples was taken, and a model was trained on this dataset.</p>
<p>The spatial distribution of these recordings is shown below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/map-initial-small-model.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">audio source locations</figcaption><p></p>
</figure>
</div>
</section>
<section id="label-verification-and-inclusion-of-edge-cases" class="level3">
<h3 class="anchored" data-anchor-id="label-verification-and-inclusion-of-edge-cases">Label verification and inclusion of edge cases</h3>
<p>I wrote a python function to help verify the labelled data. The function iterates through samples, providing the spectrogram and 3s of audio for listening, enabling me to check the labels. It took about 4-5 hours to check 1000 samples. Samples were sorted by the model’s confidence for each sample - meaning I could focus verification work on those samples which were most confidently wrong.</p>
<p>For example, if the model predicted p(present) = 0.99, and the label was 0, I’d listen to those examples and re-label them if they actually contained the target species. Similarly, if the model predicted p(present) = 0.01, and the label was 1, I’d listen to those examples and re-label them if they didn’t contain the target species. For the target present class, all of the samples were verified manually.</p>
<p>For the absent class, there were too many samples (~33,000) to check manually, so the samples were sorted in order of the model’s highest confidence of target presence, and the top 1806 samples were checked until predicted probabilities of 0.8 were reached. Within these checked samples, 231 were found to contain the target species, and were re-labelled. The remaining samples were assumed to be correctly labelled. The absent class for the verified training set was constructed from 200 of those samples which were predicted present, but actually absent, and 800 from the remaining samples, for a total of 1000 absent samples. Finally these absent samples were verified by listening to the audio and looking at spectrograms.</p>
<p>The reason I included 200 samples which the model was confidently wrong about is to give future models more examples of edge cases to learn from. These are the hard examples in <strong>hard example mining</strong>.</p>
<p>This verification process was done at various stages during dataset development, allowing me to create a more accurate set of training examples, a more accurate validation set, and finally to check the examples in the test set to re-label any obviously mis-labelled examples.</p>
</section>
<section id="custom-label-verification-process" class="level3">
<h3 class="anchored" data-anchor-id="custom-label-verification-process">Custom label verification process</h3>
<p>https://drive.google.com/file/d/17tTa0StctX1ZfcWnhtLAg8CwkOCQwNVa/view?usp=sharing</p>
<iframe src="https://drive.google.com/file/d/17tTa0StctX1ZfcWnhtLAg8CwkOCQwNVa/preview" width="640" height="480" allow="autoplay">
</iframe>
<p><em>Above: example of custom label verification process - two different brightness levels are used, and audio can be listened to.</em></p>
</section>
<section id="train-a-second-model" class="level3">
<h3 class="anchored" data-anchor-id="train-a-second-model">Train a second model</h3>
<p>A second model was trained using cleaned human labelled data, without the errors of the original human labelled dataset, allowing the model to learn from a more accurate set of training examples.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/map-second-model.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">second model training locations</figcaption><p></p>
</figure>
</div>
</section>
<section id="focal-example-bootstrapping" class="level3">
<h3 class="anchored" data-anchor-id="focal-example-bootstrapping">Focal example bootstrapping</h3>
<p>The human labelling process only looked for the first vocalization of the target species within a given time interval. This meant that the most prominent vocalizations might occur later in the recording, and would not be labelled. The second model was used to make predictions of target presence on the remainder of the recordings from within the species habitat, and these predictions were sorted by model confidence. This created a new set of examples which were close-up, focal examples of the target species which were verified then added to the training set.</p>
<p>An experimental model was trained using only focal examples, and absent examples. This produced a model with high precision but low recall compared with previous models.</p>
</section>
<section id="out-of-habitat-examples" class="level3">
<h3 class="anchored" data-anchor-id="out-of-habitat-examples">Out of habitat examples</h3>
<p>So far all the audio used had been taken from habitats where the target species had been labelled in the wildtrax database. To add training data from outside those locations, the following process was used.</p>
<ul>
<li>1000 recordings were downloaded from a random sample of all the recordings from locations not previously tagged as containing the target species. From these, 852 new locations were represented.</li>
<li>These recordings were inititally assumed not to contain vocalizatons of the target species, and so were given a label of zero meaning target absent.</li>
<li>Model 2 was used to make predictions on all of this audio, creating set of 168431 new 3 second samples.</li>
<li>The examples with p(target present) &gt; 0.8 were all checked manually, and those containing the target species vocalization were re-labelled as present.</li>
<li>Those examples with p(target present) &gt; 0.8 which did not contain the target species vocalization were assigned a label of target absent, and added to the training set. These examples allowed the model to encounter species with similar songs to the target and to learn not to classify these as present. These 1012 samples with p(present) &gt; 0.8 comprise the 0.6% of the total 168,431 out of habitat samples, and these were chosen preferentially to give the model more confusing examples to learn from.<br>
</li>
<li>A set of out-of-habitat absent examples was created by taking 1000 of those examples which produced the highest error for the model 2, and 1000 from the remaining samples with p(present) &lt; 0.8. This was further split into training, validation and test sets and combined with the existing datasets for training and evaluation of the final model.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/out-of-habitat-map.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">out-of-habitat-locations</figcaption><p></p>
</figure>
</div>
</section>
<section id="final-model-training" class="level3">
<h3 class="anchored" data-anchor-id="final-model-training">Final model training</h3>
<p>A final dataset was constructed using the following sources: - 1000 verified human labelled <strong>present</strong> examples from the original wildtrax tags - 2000 verified human labelled inferred <strong>absent</strong> examples from the original wildtrax tags - 1000 verified model extracted <strong>focal present</strong> examples from unlabelled recordings - 500 verified <strong>absent</strong> examples from outside target habitat, picked from those most confusing to a previous model. - 500 additional <strong>absent</strong> examples from outside target habitat.</p>
<p>This created a total of 2000 present and 3000 absent examples for training.</p>
<p>A resnet 34 model was trained for 20 epochs with a learning rate of 0.001.</p>
<p>Opensoundscape has image augmentation, weight decay and other good practices built into their model training pipeline. These were left at default values.</p>
</section>
<section id="process-diagram-for-dataset-creation-cleaning-and-model-training" class="level3">
<h3 class="anchored" data-anchor-id="process-diagram-for-dataset-creation-cleaning-and-model-training">Process diagram for dataset creation, cleaning and model training</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/osfl-dataset-creation-pipeline.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">dataset creation pipeline</figcaption><p></p>
</figure>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>
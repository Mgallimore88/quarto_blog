[
  {
    "objectID": "posts/embeddings/embeddings.html",
    "href": "posts/embeddings/embeddings.html",
    "title": "MG ML",
    "section": "",
    "text": "image\n\n\nIn this example I’ll make a collaborative filtering model (recommender system) which uses an entity embedding as part of a system for recommending books to users.\nEmbeddings are a neat way to take a large number of individual items (users, products, locations for example), and represent each item using an n-dimensional vector instead of using its unique id. At first this might sound like it would increase the size and complexity of the model - since each item now needs an additional vector representation - but in fact this process reduces the number of individual inputs the model needs to see to be able to make predictions.\nFor example, if we had an embedding for 1000 book titles, without an embedding layer the model would need to see each unique ID and learn the difference between them. An embedding vector for each of these book titles might be 2 dimensions deep, and might encode for each book’s sci-fi-ness and its length. This means we could feed this two dimensional embedding vector as input to the model rather than the 1000 individual titles. Since those inputs represent something real about the book, that might be enough information to make sensible predictions with. In a sense the embedding compresses information about each of the N inputs into an n dimensional vector.\nIn this blog post I’ll follow a similar process to the one outlined in the fast.ai course which used the movielens dataset. I’ll aim to explain some nuances about embedding layers, since I found this concept pretty confusing at first. Now that I’ve got my head around them I’m pretty amazed at how elegant, powerful and useful embeddings can be, and I’m excited to start trying out creative uses for embeddings.\nRead more on embeddings in this paper: Guo, Cheng et al. “Entity Embeddings of Categorical Variables”\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom fastai.collab import *\nfrom fastai.tabular.all import *\n\n\ndef display_all(df):\n    with pd.set_option('display.max_columns', 0, 'display.max_rows', 0):\n        print(df)\n\n\npath = Path('/kaggle/input/book-recommendation-dataset/')"
  },
  {
    "objectID": "posts/embeddings/embeddings.html#embeddings",
    "href": "posts/embeddings/embeddings.html#embeddings",
    "title": "MG ML",
    "section": "Embeddings",
    "text": "Embeddings\nSince there are hundreds of thousands of individual user IDs, and many more book titles, it will be useful to compress this data in some way - in a way which keeps the relevant information about each user and book, but doesn’t require the model to learn each individual user ID or book title. This is where Embeddings come in handy.\n\ndls = CollabDataLoaders.from_df(ratings, item_name='title', bs=16)"
  },
  {
    "objectID": "posts/embeddings/embeddings.html#take-a-sample",
    "href": "posts/embeddings/embeddings.html#take-a-sample",
    "title": "MG ML",
    "section": "Take a sample",
    "text": "Take a sample\nTo speed up development and testing We’ll work with a random sample of 300,000 users from the dataset.\n\nnumber_of_samples = 300000\ndf=ratings.sample(number_of_samples)\ndls = CollabDataLoaders.from_df(df, item_name='title', bs=64)"
  },
  {
    "objectID": "posts/embeddings/embeddings.html#sample-only-popular-books-and-users-with-lots-of-entries.",
    "href": "posts/embeddings/embeddings.html#sample-only-popular-books-and-users-with-lots-of-entries.",
    "title": "MG ML",
    "section": "Sample only popular books and users with lots of entries.",
    "text": "Sample only popular books and users with lots of entries.\nDeliberately selecting from the most read titles, and the most active readers could be a way of getting the information density up a little. This is definitely a design decision which should be scrutinized, since it biases the system towards more popular items, but it could be a good way to jumpstart training.\nPlus it doesn’t make a lot of sense to be training a collaborative filtering model on users who have read only one book: there wouldn’t be any second item to lookup and recommend for another user who has read the same book.\n\nbook_count = len(set(ratings.title))\npopular_books = ratings.title.value_counts()[:1000].keys()\n\nreader_count = len(set(ratings.user))\navid_readers = ratings.user.value_counts()[:1000].keys()\n\n\nlen(ratings)\n\n1031136\n\n\nOverwriting the variable dense_df with this new selection\n\ndense_df = ratings[ratings.title.isin(popular_books)]\ndense_df = (dense_df[dense_df.user.isin(avid_readers)])\nprint(len(dense_df))\n\n76402\n\n\nNow we’ve got the number of samples in the database down to 76402, and it only contains the top 1000 readers and the top 1000 books."
  },
  {
    "objectID": "posts/embeddings/embeddings.html#make-a-new-dataloaders-object-to-draw-training-and-validation-samples-from-this-new-dataframe.",
    "href": "posts/embeddings/embeddings.html#make-a-new-dataloaders-object-to-draw-training-and-validation-samples-from-this-new-dataframe.",
    "title": "MG ML",
    "section": "Make a new dataloaders object to draw training and validation samples from this new dataframe.",
    "text": "Make a new dataloaders object to draw training and validation samples from this new dataframe.\n\ndense_dls = CollabDataLoaders.from_df(dense_df, item_name='title', bs=64)\nn_users = len(dense_dls.classes['user'])\nn_titles = len(dense_dls.classes['title'])\n\n\nmodel = DotProduct(n_users, n_titles, n_factors=50)"
  },
  {
    "objectID": "posts/embeddings/embeddings.html#lets-see-how-the-model-trains-now",
    "href": "posts/embeddings/embeddings.html#lets-see-how-the-model-trains-now",
    "title": "MG ML",
    "section": "Let’s see how the model trains now",
    "text": "Let’s see how the model trains now\n\nlearn = Learner(dense_dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      0.169560\n      0.172441\n      00:06\n    \n    \n      1\n      0.119379\n      0.122923\n      00:06\n    \n    \n      2\n      0.103940\n      0.114699\n      00:06\n    \n    \n      3\n      0.098195\n      0.113169\n      00:06\n    \n    \n      4\n      0.099816\n      0.113051\n      00:06\n    \n  \n\n\n\n\nGreat - the training only takes 5s per epoch, and we’re still seeing convergence after 5 epochs. Let’s try to improve from here"
  },
  {
    "objectID": "posts/embeddings/embeddings.html#thinking-about-latent-factors-as-components-of-a-vector-in-an-n-dimensional-feature-space",
    "href": "posts/embeddings/embeddings.html#thinking-about-latent-factors-as-components-of-a-vector-in-an-n-dimensional-feature-space",
    "title": "MG ML",
    "section": "Thinking about latent factors as components of a vector in an n-dimensional feature space",
    "text": "Thinking about latent factors as components of a vector in an n-dimensional feature space\nHere are the factors for each of the users in the batch:\n\nmodel.user_factors(batch[:,0])\n\ntensor([[ 0.0206,  0.0324, -0.0256,  ..., -0.0972,  0.0861, -0.0837],\n        [ 0.0978,  0.1441,  0.0092,  ..., -0.0022, -0.0379,  0.0745],\n        [-0.0334,  0.0943,  0.0024,  ...,  0.0780, -0.0867, -0.0408],\n        ...,\n        [ 0.0150,  0.0170,  0.0297,  ...,  0.0246, -0.0477, -0.0407],\n        [-0.0341, -0.0660, -0.0711,  ...,  0.0390,  0.0392,  0.0218],\n        [ 0.0929,  0.0700,  0.0584,  ...,  0.0826, -0.0621,  0.0504]],\n       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n\n\nEach of these numbers represents a learned latent factor for that user. The latent factors can can be thought of as the contribution / component to a vector in n-dimensional space, where each number is a different axis’s contribution. The factors are all orthoganal to oneanother. They can represent things like taste, genre, age etc.\nFor example: if user A has 3 latent factors x, y, z, and these have values 1, 0.2, -0.9, then we can imagine a vector in 3d space which extends along the x dimension by 1, along y by 0.2, and extends negatively along the z dimension by 1.\nAnother user, or book title, might point in a very similar direction. This would mean that their factors overlap a lot and tend not to cancel out.\nEach of these dimensions could code for something like ‘enjoys horror books’, ‘enjoys shorter books’, younger.\nIf there was another user who’s factors were -1, 0.2, 1, we might say that they had the opposite taste for horror stories, that they have the same liking for shorter books, and that they are older.\nThe latent factors encode for real world meaning, but the factors themselves aren’t chosen by the engineer when setting up the neural network - rather they emerge from the relationships between books, users and ratings as the model trains."
  },
  {
    "objectID": "posts/embeddings/embeddings.html#finding-the-books-with-the-highest-bias",
    "href": "posts/embeddings/embeddings.html#finding-the-books-with-the-highest-bias",
    "title": "MG ML",
    "section": "Finding the books with the highest bias",
    "text": "Finding the books with the highest bias\nHere’s a list of books with a high bias: they end up having a higher rating across the board, despite the specific features which were learned to describe the books. Intuitively this means that they’re high quality - since they get consistently high ratings despite their genre and the users’ tastes.\n\nbooks_bias = learn.model.title_bias.weight.squeeze()\nidxs = books_bias.argsort(descending=True)[:20]\n[dense_dls.classes['title'][i] for i in idxs]\n\n['Harry Potter and the Order of the Phoenix (Book 5)',\n 'Harry Potter and the Prisoner of Azkaban (Book 3)',\n \"Harry Potter and the Sorcerer's Stone (Book 1)\",\n 'Harry Potter and the Chamber of Secrets (Book 2)',\n 'To Kill a Mockingbird',\n '84 Charing Cross Road',\n \"Harry Potter and the Sorcerer's Stone (Harry Potter (Paperback))\",\n 'The Lovely Bones: A Novel',\n 'A Wrinkle in Time',\n 'Harry Potter and the Goblet of Fire (Book 4)',\n 'The Little Prince',\n 'The Secret Garden',\n 'The Da Vinci Code',\n 'Girl in Hyacinth Blue',\n 'Stupid White Men ...and Other Sorry Excuses for the State of the Nation!',\n 'Lord of the Flies',\n 'Dragonfly in Amber',\n \"Dude, Where's My Country?\",\n 'Fahrenheit 451',\n 'Carrie']"
  },
  {
    "objectID": "posts/kaggle-insurance-competition/insurance_competition.html#checkpoint",
    "href": "posts/kaggle-insurance-competition/insurance_competition.html#checkpoint",
    "title": "MG ML",
    "section": "Checkpoint",
    "text": "Checkpoint\n\nsave_pickle(path/'to.pkl', to)\n\n\nto = load_pickle(path/'to.pkl')"
  },
  {
    "objectID": "posts/kaggle-insurance-competition/insurance_competition.html#visualizing-the-data-splits-made-by-the-initial-decision-tree",
    "href": "posts/kaggle-insurance-competition/insurance_competition.html#visualizing-the-data-splits-made-by-the-initial-decision-tree",
    "title": "MG ML",
    "section": "Visualizing the data splits made by the initial decision tree",
    "text": "Visualizing the data splits made by the initial decision tree\n\nsample_idx = np.random.permutation(len(y))[:1000]\nviz_rmodel = dtreeviz.model(model=m, X_train=xs.iloc[sample_idx], y_train=y.iloc[sample_idx], feature_names=xs.columns, target_name=dep_var)\nviz_rmodel.view(orientation='LR')\n\n/Users/mikeg/miniforge3/envs/fastai/lib/python3.10/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n\n\n\n\n\nThe ordering of boolean columns such as Age_na for whether the age column was missing is preserved, with False=1 and True=2\n\ndraw_tree(m, xs, size=10, leaves_parallel=True, precision=2)\n\n\n\n\nThe decision tree is making some splits which result in groups with a larger error in one group and a smaller error in the other group. The group with the smaller error is always the largest of the two groups. This is probably ok, since the model is trying to find the split which gives the largest overall reduction in the squared error across the two groups.\nThe previous examples i’ve worked on though, had splits which reduced the error in both groups - so pay attention to this when interpreting the results. At the end of the decision tree, the average of the weighted errors in the leaf nodes needs to be less than the average of the errors in the parent node, otherwise the model is worse than just predicting the mean."
  },
  {
    "objectID": "posts/kaggle-insurance-competition/insurance_competition.html#initial-observations",
    "href": "posts/kaggle-insurance-competition/insurance_competition.html#initial-observations",
    "title": "MG ML",
    "section": "Initial observations",
    "text": "Initial observations\nAnnual income is a strong predictor of premium amount the group earning under 46k has a higher premium amount than the group earning over 46k. For the group earning over 46k, the prediction of premium price for the average of this group is worse than the mean prediction for the entire dataset, but the group only contains 205775 samples compared with 754225 in the group earning under 46k where the prediction for the mean of the group is better than the mean prediction for the entire dataset.\nFor the group earning under 46k, the next most infomative split is whether their income amount is provided or not. For those where the income amount isn’t provided, the next most informative split is credit score. For those with income amount provided (as some amount below 46k) this ends in a leaf node where the squared error for the group of 720,000 people is 1.0 compared with 1.21 at the parent node. Further splits in the tree will lead to finer categories, potentially some splits for this lower income group will to a further reduction in the error.\nOther interesting splits are: - If the number of previous claims is 2 or more, the premium amount is higher than if the number of previous claims is 1 or 0. - Policy holders who have just joined pay a higher premium than those who have held a policy for longer. - Policy holders with a higher health score pay more than those with a lower health score. - Policy holders with a high credit score pay more than those with a low credit score.\nhow many missing incomes are there?\n\nto.xs.loc[to.xs['Annual Income_na'] == 1]['Annual Income'].describe()\n\ncount    1.155051e+06\nmean     3.274522e+04\nstd      3.217951e+04\nmin      1.000000e+00\n25%      8.001000e+03\n50%      2.391100e+04\n75%      4.463400e+04\nmax      1.499970e+05\nName: Annual Income, dtype: float64\n\n\n\nto.xs.loc[to.xs['Annual Income_na'] == 2]['Annual Income'].describe()\n\ncount    44949.000000\nmean     23897.001953\nstd          0.000000\nmin      23897.000000\n25%      23897.000000\n50%      23897.000000\n75%      23897.000000\nmax      23897.000000\nName: Annual Income, dtype: float64\n\n\nHere we can see that in the columns where the income is not provided, the income amount is set to the average of the income column. There are 50,000 missing values in the income column compared with 1.15 million present values. 0.43% of the values are missing. We shouldn’t worry about this 5% too much."
  },
  {
    "objectID": "posts/kaggle-insurance-competition/insurance_competition.html#rmse",
    "href": "posts/kaggle-insurance-competition/insurance_competition.html#rmse",
    "title": "MG ML",
    "section": "RMSE",
    "text": "RMSE\nMake a function to calculate the model’s root mean square error on the predictions.\nWe’ve already taken the log of the premium amounts in the training and validation sets, so this is really RMSLE.\n\ndef r_mse(pred, y): return math.sqrt(((pred-y)**2).mean())\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n\n\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n(0.0, 1.5384979288903713)\n\n\nAn error of 0 on the training set, and 1.5 on the validation set is bad news! We are overfitting enormously to the training data, and the model doesn’t generalize at all well on the validation set. The reason is that we have almost as many leaf nodes to the tree as there are data points in the training set. This means that we’re being too specific in our predictions, and not leveraging the ability to average across items in a leaf node.\n\nm.get_n_leaves(), len(xs)\n\n(950988, 960000)\n\n\n\nm=DecisionTreeRegressor(min_samples_leaf=50)\nm.fit(xs, y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)\n\n(0.9963146137544396, 1.0883776194426706)\n\n\nThat’s better - the model can now generalize to the validation set with an error which is better than just predicting the mean of all the values. 1.2 vs 1.09."
  },
  {
    "objectID": "posts/BCE-model/BCEmodel.html",
    "href": "posts/BCE-model/BCEmodel.html",
    "title": "MG ML",
    "section": "",
    "text": "leaves\n\n\nIn this notebook we build and train a convolutional neural network based classifier model in PyTorch recognize images of canoes, kayaks and sailboats. We use the fastai library to speed up the process and get us to a working model with less code.\nThis notebook was used for a presentation at ML Squamish to take first time coders from a no-code background to training their first image recognition model in an evening.\nWe explore some different loss functions and training approaches.\nTopics covered: - Training and validation sets - Image augmentations - Cross entropy loss - Binary Crossentropy Loss - Learning rate finder\nLet’s get started!"
  },
  {
    "objectID": "posts/BCE-model/BCEmodel.html#choose-a-set-of-categories-for-the-classification-model",
    "href": "posts/BCE-model/BCEmodel.html#choose-a-set-of-categories-for-the-classification-model",
    "title": "MG ML",
    "section": "Choose a set of categories for the classification model",
    "text": "Choose a set of categories for the classification model\n\nboat_types = 'canoe', 'kayak', 'sailboat'\n\n\npath = Path('boats')\n\nDownload images from chosen categories.\nThis cell may take a few minutes to run\n\nif not path.exists():\n    path.mkdir()\n    for boat in boat_types:\n        dest = (path/boat)\n        dest.mkdir(exist_ok=True)\n        results = search_images(boat)\n        download_images(dest, urls=results)\n\n\n!ls {path}\npath\n\ncanoe  kayak  sailboat\n\n\nPath('boats')\n\n\n\nfilenames = get_image_files(path)\nfilenames\n\n(#554) [Path('boats/sailboat/3b873c96-0d1f-4404-9aae-af25d3805177.jpg'),Path('boats/sailboat/6336c57d-8ad9-4318-836c-fba58b5357be.jpg'),Path('boats/sailboat/d2a981d0-0d3e-4b0e-a077-a5cbda1a41b0.JPG'),Path('boats/sailboat/da7dc60e-7c83-4807-9fc1-a0f5061a5080.jpg'),Path('boats/sailboat/168643af-803b-4783-8c6b-a785157c481e.jpeg'),Path('boats/sailboat/f4055206-199c-42a8-bf74-8ed6c4dc7e08.jpg'),Path('boats/sailboat/d5883260-5325-41cb-a645-016051c37819.jpg'),Path('boats/sailboat/4b44e5d8-75e1-413b-a9b9-bd3f19b63df4.jpg'),Path('boats/sailboat/86d8f1b9-b75b-467f-8476-2d91805a150b.jpg'),Path('boats/sailboat/e07b4c1d-a717-4c10-9059-005016798081.jpeg')...]"
  },
  {
    "objectID": "posts/BCE-model/BCEmodel.html#some-of-the-terminology-in-pytorch-fastai-can-be-confusing.-they-can-have-similar-names-which-mean-very-specific-things.-here-are-some-definintions",
    "href": "posts/BCE-model/BCEmodel.html#some-of-the-terminology-in-pytorch-fastai-can-be-confusing.-they-can-have-similar-names-which-mean-very-specific-things.-here-are-some-definintions",
    "title": "MG ML",
    "section": "Some of the terminology in PyTorch fastai can be confusing. They can have similar names which mean very specific things. Here are some definintions",
    "text": "Some of the terminology in PyTorch fastai can be confusing. They can have similar names which mean very specific things. Here are some definintions\nDataset: an iterable over tuples containing images with their corresponding category.  Datasets: a fastai class which joins together a training dataset and a validation dataset into one object. DataLoader: a PyTorch iterable returning a batch of datasets. DataLoaders: a fastai iterable which splits dataloaders into training and validation datasets.  batch: The sample of the dataset loaded in parallel and passed to the model during one training loop."
  },
  {
    "objectID": "posts/BCE-model/BCEmodel.html#use-the-fastai-datablock-to-create-a-way-to-load-datasets-and-dataloaders",
    "href": "posts/BCE-model/BCEmodel.html#use-the-fastai-datablock-to-create-a-way-to-load-datasets-and-dataloaders",
    "title": "MG ML",
    "section": "Use the fastai DataBlock to create a way to load datasets and dataloaders",
    "text": "Use the fastai DataBlock to create a way to load datasets and dataloaders\n\nboats = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128)\n)\n\n\ndataloaders = boats.dataloaders(path)\n\nThe dataloaders object provides a way of loading batches of data from a location. It contains specific details on how the data should be transformed, how big a validation set to make, and a bunch of helper functions such as show.batch() which allow quick visualization and troubleshooting."
  },
  {
    "objectID": "posts/BCE-model/BCEmodel.html#training-and-validation-sets",
    "href": "posts/BCE-model/BCEmodel.html#training-and-validation-sets",
    "title": "MG ML",
    "section": "Training and validation sets",
    "text": "Training and validation sets\nOur data is split into training and validation sets. 20% of the data is in the validation set, and 80% is in the training set.\nHere we can see a sample of one batch from the training set. The input feature is an image of a boat, and the target label is the class of the boat. There are three classes: canoe, sailboat and kayak. We can see using the show_batch function that the images and labels are all loaded and re-sized correctly.\n\ndataloaders.train.show_batch()"
  },
  {
    "objectID": "posts/BCE-model/BCEmodel.html#pre-trained-models.",
    "href": "posts/BCE-model/BCEmodel.html#pre-trained-models.",
    "title": "MG ML",
    "section": "Pre-trained models.",
    "text": "Pre-trained models.\nHere we take a pre-trained image recognition model and fine tune it to learn the classes in our dataset. This resnet18 model was originally trained on the ImageNet dataset to recognize over 20,000 classes, with hundreds of examples in each class. We can take advantage of the large amount of work which went into training this model.\nUnder the hood fastai replaces the output layers of the pre-trained model with randomly initialized layers with the same number of output nodes as the number of classes we’re trying to predict (three: ‘canoe’, ‘kakak’, ‘sailboat’). When we re-train the model, most of the work goes into training these final layers of the network, until the model learns to represent the categories we’re looking for. This works even when the category we’re looking for wasn’t seen by the pre-trained model’s original dataset.\nThe earlier layers of a neural network tend to learn things like gradients, edges, colour, and the semantic meaning increases in deeper layers until the final layer represents the concept level output: canoe, kayak, sailboat. These earlier layers are useful for picking out features in a broad set of images - not just the ones the model was trained on."
  },
  {
    "objectID": "posts/BCE-model/BCEmodel.html#metrics",
    "href": "posts/BCE-model/BCEmodel.html#metrics",
    "title": "MG ML",
    "section": "Metrics",
    "text": "Metrics\nMetrics are a human readable representation of the performance of our model. Error rate is a pretty simple and easy to understand metric: what portion of the examples from the validation set did we mis-classify."
  },
  {
    "objectID": "posts/BCE-model/BCEmodel.html#loss",
    "href": "posts/BCE-model/BCEmodel.html#loss",
    "title": "MG ML",
    "section": "Loss",
    "text": "Loss\nLoss can be thought of as a sort of punishment function. The training process seeks to minimize loss, and the gradient of the loss function is used to calculate a better set of parameters during model training. The loss function can be thought of as a kind of error measure, but it is designed to be read by a machine, not a human, and to have smooth gradients. Lower loss usually means a better model. When plotting loss per epoch, decreasing loss curves are a good sign which mean the model is improving. If the loss curves are noisy or increasing, it is an indicator that we might have picked too high a learning rate, or that the model is beginnign to over-fit.\n\nlearn = vision_learner(dataloaders, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\nlearn.recorder.plot_loss()\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      1.571849\n      0.666472\n      0.163636\n      00:14\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      error_rate\n      time\n    \n  \n  \n    \n      0\n      0.534734\n      0.270972\n      0.118182\n      00:15\n    \n    \n      1\n      0.436475\n      0.234348\n      0.081818\n      00:15\n    \n    \n      2\n      0.353536\n      0.214399\n      0.063636\n      00:15\n    \n    \n      3\n      0.313201\n      0.206994\n      0.063636\n      00:14"
  },
  {
    "objectID": "posts/BCE-model/BCEmodel.html#great-again-we-got-a-sensible-preciction-of-88-probability-of-a-canoe.",
    "href": "posts/BCE-model/BCEmodel.html#great-again-we-got-a-sensible-preciction-of-88-probability-of-a-canoe.",
    "title": "MG ML",
    "section": "Great, again we got a sensible preciction of 88% probability of a canoe.",
    "text": "Great, again we got a sensible preciction of 88% probability of a canoe.\n\nprediction, index, probs = learn.predict('images/leaves.jpg')\nprint(f\"The model predicted {prediction} with a confidence of {probs[index]}\")\nprint(probs)\nImage.open('images/leaves.jpg')\n\n\n\n\n\n\n\n\nThe model predicted canoe with a confidence of 0.5352954268455505\nTensorBase([0.5353, 0.1248, 0.3399])"
  },
  {
    "objectID": "posts/BCE-model/BCEmodel.html#first-we-need-to-create-a-function-to-get-the-label",
    "href": "posts/BCE-model/BCEmodel.html#first-we-need-to-create-a-function-to-get-the-label",
    "title": "MG ML",
    "section": "First we need to create a function to get the label",
    "text": "First we need to create a function to get the label\nWe already have this, in the form of parent_label(), which returns a string version of the parent folder containing the image. However, we’re also going to tell fastai that the labels are multi-category labels, even though they’re not. The multi-category block expects a list of strings as input - so if we just pass in a string then that will get split into a list of characters - not what we want.\nI’m doing this so that the labels are converted to one hot encoded vectors, which will allow us to use binary cross-entropy loss.\nSpecifying the target type as ‘MultiCategoryBlock’ in the fastai DataBlock will create a model which can handle multiple classes in a single image. Rather than strongly picking one class out of many, we’ll be choosing potentially many classes out of many. This has the beneficial side effect that we can threshold the output activations and create a ‘no class recognized’ output - an ability we don’t have when using cross-entropy loss across all the activations.\n\ndef get_y(r): return parent_label(r).split(' ')\n\n\ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    \"Compute accuracy when `inp` and `targ` are the same size.\"\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp>thresh)==targ.bool()).float().mean()\n\n\nboat_dls = DataBlock(\n    blocks=(ImageBlock, MultiCategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=get_y,\n    item_tfms=Resize(128),\n    batch_tfms=aug_transforms()\n).dataloaders(path)\n\n\nlr = learn.lr_find()\n\n\n\n\n\n\n\n\n\n\n\n\nlearn = vision_learner(boat_dls, resnet18, pretrained=True, metrics=accuracy_multi)\n%time learn.fine_tune(6, base_lr=lr[0])\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      1.020967\n      0.877902\n      0.563636\n      00:13\n    \n  \n\n\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      accuracy_multi\n      time\n    \n  \n  \n    \n      0\n      0.947158\n      0.818463\n      0.545455\n      00:14\n    \n    \n      1\n      0.937178\n      0.750471\n      0.572727\n      00:13\n    \n    \n      2\n      0.897594\n      0.689925\n      0.627273\n      00:14\n    \n    \n      3\n      0.863008\n      0.640909\n      0.654545\n      00:14\n    \n    \n      4\n      0.827849\n      0.622816\n      0.663636\n      00:13\n    \n    \n      5\n      0.808169\n      0.619115\n      0.660606\n      00:14"
  },
  {
    "objectID": "posts/RAG/RAG.html",
    "href": "posts/RAG/RAG.html",
    "title": "MG ML",
    "section": "",
    "text": "Simple RAG Example\nRetreival Augmented Generation\nhttps://parlance-labs.com/education/rag/ben.html\n\n\n\nrag-tree.webp\n\n\n\nRAG is Retreival Augmented Generation.\nIt just means ‘provide relevant context’\nIt works by\n\n\ncreating an embedding from a prompt\ncreating embeddings from sections of a document\nfinding the cosine similarity between the prompt and each section of the document.\n\n-Once the paragraph with the hightest cosine similarity to the prompt is found, the top 3 sentences are fed into a generative model to generate an answer.\n\n# %pip install -U sentence-transformers\n# $pip install wikipedia-api\n# %pip install claudette\n\n\nfrom sentence_transformers import SentenceTransformer\nfrom wikipediaapi import Wikipedia\nfrom claudette import Chat, models\n\n/Users/mikeg/miniforge3/envs/ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nI got the model list from here https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n\n# model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\nmodel = SentenceTransformer('Alibaba-NLP/gte-base-en-v1.5', trust_remote_code=True)\n\n\n\nFetch some text content and embed it\n\nwiki = Wikipedia('RAGBot/0.0', 'en')\ndoc = wiki.page('Albert Einstein').text\nparagraphs = doc.split('\\n\\n')\n# ... make embedding\ndocs_embed = model.encode(paragraphs, normalize_embeddings=True)\n\n\n\nMake an embedding of the prompt (query)\n\nquery = \"What gave Einstein the chance to meet his fellow neuroanatomist Ramon?\"\nquery_embed = model.encode(query, normalize_embeddings=True)\nquery_embed.shape\n\n(768,)\n\n\n\nimport numpy as np\nsimilarities = np.dot(docs_embed, query_embed)\nsimilarities\n\narray([0.5728152 , 0.47459427, 0.44192564, 0.47382304, 0.5413194 ,\n       0.43490496, 0.4756278 , 0.5182402 , 0.48718232, 0.41080484,\n       0.37724665, 0.57194126, 0.50365496, 0.46164155, 0.57233626,\n       0.5127679 , 0.52641445, 0.5398744 , 0.6040658 , 0.4396679 ,\n       0.47936997, 0.5163653 , 0.5112858 , 0.5028564 , 0.48975816,\n       0.40509236, 0.55507207, 0.5173719 , 0.47807235, 0.40626654,\n       0.3443527 , 0.39776003, 0.49057645, 0.5369898 , 0.33842805,\n       0.39533782, 0.31863862, 0.4034021 , 0.43526107, 0.42657036,\n       0.41576654, 0.44364488, 0.3890611 , 0.40116668, 0.50536174,\n       0.44676283, 0.3784415 , 0.44369176, 0.43291909, 0.42396727,\n       0.46925837, 0.37816215, 0.44824862, 0.48065543, 0.42301148,\n       0.37530896, 0.46815017, 0.4325635 , 0.3740057 , 0.53968686,\n       0.40272278, 0.37232846, 0.40951195, 0.42262912, 0.47417068,\n       0.46981564, 0.39782795, 0.31879896, 0.33717418], dtype=float32)\n\n\n\ntop_3_idx = np.argsort(similarities)[::-1][:3]\n\n\ntop_3_idx\n\narray([18,  0, 14])\n\n\n\nmost_similar_documents = [paragraphs[idx] for idx in top_3_idx]\n\n\nllm = models[2]\nchat = Chat(llm, sp=f\"Here is some information from Wikipedia, it will help you to answer a question. Wikipedia information: {str(most_similar_documents)}\")\n\n\nchat(query)\n\nAccording to the information provided, Einstein’s 1923 visit to Spain gave him the chance to meet the fellow Nobel laureate, the neuroanatomist Santiago Ramón y Cajal. The passage states:\n“(His Spanish trip also gave him a chance to meet a fellow Nobel laureate, the neuroanatomist Santiago Ramón y Cajal.)”\nSo Einstein’s 1923 trip to Spain provided him the opportunity to meet and interact with the Nobel laureate neuroanatomist Santiago Ramón y Cajal.\n\n\nid: msg_01Wu1ME2c2Chmg5x9Ej2Tohm\ncontent: [{'text': 'According to the information provided, Einstein\\'s 1923 visit to Spain gave him the chance to meet the fellow Nobel laureate, the neuroanatomist Santiago Ramón y Cajal. The passage states:\\n\\n\"(His Spanish trip also gave him a chance to meet a fellow Nobel laureate, the neuroanatomist Santiago Ramón y Cajal.)\"\\n\\nSo Einstein\\'s 1923 trip to Spain provided him the opportunity to meet and interact with the Nobel laureate neuroanatomist Santiago Ramón y Cajal.', 'type': 'text'}]\nmodel: claude-3-haiku-20240307\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 1957, 'output_tokens': 119}\n\n\n\n\n\n\nImprovements\nCross Encoder - This method use a Bi-encoder model. It is very efficient, since the embeddings for the document can be computed in parallel, and stored in a database, so that in the end all that you need to look up is the similarity between the prompt embedding and the embeddings stored in a database.\nThe documents and query representations are computed entirely separately in the bi-encoder, so they aren’t aware of each other. Improvements can be made by using a Cross-encoder model. These essentially are a binary classifier, where p(positive class) is taken as the similarity score. These are slower than bi-encoders, but are more accurate.\n\nReranking Cross encoders are computationally expensive to run, so using a cross-encoder on the entire set of documents, for every prompt, would take a long time. One solution is to return a shortlist of documents using a computationally efficient approach, such as a bi-encoder, and then re-rank these by using a cross-encoder. There’s a library for this - github.com/answerdotai/rerankers\nKeyword Search always have keyword search and full text search in the pipeline.\ntf-idf term frequency-inverse document frequency weighs down common words and weighs up rare words.\nBM25 is a way to implement tf-idf."
  },
  {
    "objectID": "posts/linear-nonlinear-neuralnet/linear-nonlinear-neuralnet.html",
    "href": "posts/linear-nonlinear-neuralnet/linear-nonlinear-neuralnet.html",
    "title": "MG ML",
    "section": "",
    "text": "squeeze\n\n\nTopics covered: - PyTorch - Feature Crosses - Linear and nonlinear models - universal approximation theorem - interpreting loss curves\nIn this example, I asked chat GPT to generate a PyTorch training loop for a linear model, from a synthetic dataset. The code produced was great. It gave me ideas and some framework code from which to develop my own understanding. Modifying and developing the code was a quick way to explore some new functions and explore some ideas in an interactive way.\nI wanted to get an intuitive understanding of the universal approximation theorem, in order to be able to better decide when to choose a neural network, and when to choose another type of model."
  },
  {
    "objectID": "posts/linear-nonlinear-neuralnet/linear-nonlinear-neuralnet.html#define-a-nonlinear-model",
    "href": "posts/linear-nonlinear-neuralnet/linear-nonlinear-neuralnet.html#define-a-nonlinear-model",
    "title": "MG ML",
    "section": "Define a nonlinear model",
    "text": "Define a nonlinear model\nThis model will contain two linear layers connected by a ReLU activation function, enabling it to represent nonlinear functions.\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(10,10),\n    torch.nn.ReLU(),\n    torch.nn.Linear(10,1)\n)\n\n\n# Define the optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n\noptimizer\n\nSGD (\nParameter Group 0\n    dampening: 0\n    differentiable: False\n    foreach: None\n    lr: 0.01\n    maximize: False\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)\n\n\n\n# Define the data loader\nloader = DataLoader(train_dataset, batch_size=500, shuffle=True)\n\n\ntrain(100, loader, model, val_x, val_y)\n\n2.518, 1.761\n\n\n\n\n\nThe model converges towards a stable value, but it takes a long time and never reaches zero. This can be explained by the model not describing the mapping function perfectly - it is almost but not quite describing the equation\n\\(y=2X_0X_1 + 2X_0 -3X_1 - 3\\)\nSince we increased the number of layers in the model, interpreting what is going on by looking at the parameters has gone from being trivial to being very difficult. We are looking for coefficients which represent a straight multiplication of \\(2 * X_0\\), \\(-3 * X_1\\) and some way of representing \\(2*X_0*X_1\\)\n\nlist(model.parameters())\n\n[Parameter containing:\n tensor([[-6.0341e-01,  7.4068e-01,  1.1714e-01,  3.0487e-01,  1.1350e-01,\n           1.6252e-01, -3.8328e-01,  2.9286e-01, -1.7268e-01, -1.1349e-01],\n         [-9.7725e-01,  1.2587e+00, -9.1938e-02, -1.3213e-01,  2.7093e-02,\n          -1.1980e-01,  9.9834e-02, -2.4829e-01,  1.9772e-01, -8.2101e-03],\n         [-4.7922e-01,  5.4561e-01,  1.7990e-01, -1.3366e-01, -5.8698e-02,\n           2.0070e-01,  1.2817e-01,  2.6042e-02,  7.4049e-02,  1.5992e-02],\n         [-1.1129e-01, -3.4661e-01,  1.5593e-02, -3.3927e-02,  3.3553e-03,\n          -6.7739e-02,  1.9699e-01, -2.1908e-01, -1.9680e-01,  1.2308e-01],\n         [ 6.4029e-04,  1.1276e-01, -2.5297e-01,  2.1831e-01, -1.1878e-01,\n          -1.4201e-01,  3.0204e-01, -1.6409e-01, -1.6396e-01,  1.7286e-01],\n         [-3.0487e-01,  2.8155e-01,  9.8999e-02, -2.4160e-01, -2.4192e-01,\n          -1.1160e-01, -2.1963e-02,  7.1207e-03,  2.8266e-01,  1.1177e-01],\n         [-4.3329e-03,  2.6280e-01, -2.4909e-01, -1.4999e-01,  2.7082e-01,\n          -3.0941e-01, -4.8501e-02,  1.5773e-01, -2.4637e-01,  2.7581e-01],\n         [ 8.0356e-02, -5.0721e-01,  1.0275e-02, -1.4435e-01, -2.3078e-01,\n           1.5244e-01, -2.5871e-01,  2.0775e-01,  1.6999e-02,  1.4378e-01],\n         [-8.0510e-01,  9.3808e-01,  2.7476e-02,  4.1569e-02, -8.0434e-02,\n          -8.6201e-02, -5.4608e-02,  6.5882e-02, -3.5787e-01, -6.5858e-02],\n         [-1.0937e-01, -4.4521e-01,  8.5424e-02, -1.5546e-01, -6.3826e-02,\n           3.4240e-02, -2.7848e-01,  2.5718e-01, -1.5809e-01, -8.6447e-02]],\n        requires_grad=True),\n Parameter containing:\n tensor([ 0.0198,  0.4470,  0.2590,  0.2427,  0.2077,  0.0263, -0.1412,  0.2742,\n          0.1831,  0.1076], requires_grad=True),\n Parameter containing:\n tensor([[-0.9906, -1.6195, -0.7937,  0.3644, -0.0268, -0.4274, -0.0493,  0.2982,\n          -1.2422,  0.3623]], requires_grad=True),\n Parameter containing:\n tensor([-0.2686], requires_grad=True)]\n\n\nPerhaps we can work out the correct set of parameters for the model just by thinking about it…\nThe first node of the first layer needs to carry through the exact values of \\(X_0\\) and \\(X_1\\) so they can be used later.\nWe also need these \\(X_0\\) and \\(X_1\\) features to be multiplied by the weights 2 and -3 for the linear part of the problem.\nWe’d need an overall bias of -3, which is added to the output layer of the model.\nFinally need a point in the model where \\(X_0\\) and \\(X_1\\) are multiplied together. This never happens - unless we have a quadratic activation function. We’re providing nonlinearities using ReLUs (rectified linear units). A ReLU is like two linear functions (y=x and y=0) joined at a discontinuity at x=0.\n\nk = torch.linspace(-100, 100, 20000)\nrelu = torch.relu(k)\nplt.plot(k, relu)\nplt.title('Using a ReLU activation function after each node in a linear layer \\nis one way to add a nonlinearity to a neural network')\n\nplt.show()\n\n\n\n\n\ntrain(5000, loader, model, val_x, val_y)\n\n0.172, 0.177\n\n\n\n\n\nThis model is trained pretty well. Let’s make a plot of y vs model(x) and see what shape we get.\n\nplt.scatter(model(X).detach().numpy(), y, alpha=0.05)\n\n<matplotlib.collections.PathCollection at 0x1378a83a0>\n\n\n\n\n\nThere are outliers visible. After training for 5000 epochs, the model has found a way of approximating our nonlinear function using a combination of coefficients and nonlinear activation functions. It isn’t perfect though - since functions which build the model are all linear additions and multiplications by ReLU."
  },
  {
    "objectID": "posts/linear-nonlinear-neuralnet/linear-nonlinear-neuralnet.html#make-the-synthetic-dataset",
    "href": "posts/linear-nonlinear-neuralnet/linear-nonlinear-neuralnet.html#make-the-synthetic-dataset",
    "title": "MG ML",
    "section": "Make the synthetic dataset",
    "text": "Make the synthetic dataset"
  },
  {
    "objectID": "posts/linear-nonlinear-neuralnet/linear-nonlinear-neuralnet.html#define-a-nonlinear-model-1",
    "href": "posts/linear-nonlinear-neuralnet/linear-nonlinear-neuralnet.html#define-a-nonlinear-model-1",
    "title": "MG ML",
    "section": "Define a nonlinear model",
    "text": "Define a nonlinear model\nThis model will contain two linear layers connected by a ReLU activation function, enabling it to represent nonlinear functions.\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1,10),\n    torch.nn.ReLU(),\n    torch.nn.Linear(10,10),\n    torch.nn.ReLU(),\n    torch.nn.Linear(10,1),\n)\n\n\n# Define the optimizer\noptimizer = torch.optim.Adam(model.parameters())\n\n\n# Define the data loader\nloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n\n\ntrain(500, loader, model, val_x, val_y)\n\n0.000, 0.000\n\n\n\n\n\n\nplt.scatter(x,y)\nyhat = model(x).detach().numpy()\nplt.scatter(x, yhat)\n\n<matplotlib.collections.PathCollection at 0x297771ab0>\n\n\n\n\n\nI haven’t been able to get the model to train sufficiently for this problem. Here’s another blog post showing that it is possible, and that the resulting model doesn’t have any smooth segments on its curve:\nhttps://machinelearningmastery.com/neural-networks-are-function-approximators/"
  },
  {
    "objectID": "posts/probability-distributions/Probability.html",
    "href": "posts/probability-distributions/Probability.html",
    "title": "MG ML",
    "section": "",
    "text": "dice\n\n\nWhat happens when we add two sets of data with rectangular probability distributions?\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nn_samples = 100000\nhist_bins = 200\n\n\ndata_1 = np.random.rand(n_samples)\nplt.hist(data_1, bins=hist_bins)\nplt.show()\n\n\n\n\n\ndata_2 = np.random.rand(n_samples)\ndata_2 = data_1 + data_2\nplt.hist(data_2, bins=hist_bins)\nplt.show()"
  },
  {
    "objectID": "posts/probability-distributions/Probability.html#now-we-see-a-bell-curve.",
    "href": "posts/probability-distributions/Probability.html#now-we-see-a-bell-curve.",
    "title": "MG ML",
    "section": "Now we see a bell curve.",
    "text": "Now we see a bell curve.\nThe shape of this curve can be understood by looking at a table of the odds of each roll combination of three six-sided dice:\n\n\n\n“dice3d6”"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mike’s Machine Learning Blog",
    "section": "",
    "text": "What is an embedding layer and how does it work?\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCNN Birdsong Classifer Evaluation\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCNN Birdsong Classifier Description\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nKaggle insurance competition 2024\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBuilding a boat recognizer using the fast.ai library\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSimple RAG Example\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nExploring the capability of neural networks to solve simple linear and nonlinear algebraic equations.\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nExploring probability distributions\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m a software engineer living in Squamish BC, Canada. I moved out here to pursue a life in the mountains.\nA graduate of Acoustics, I have experience in electronics, digital signal processing, mechanical engineering, and a good math background.\nI am proficient in Python. I studied Machine Learning through the FastAI course and textbooks. I’ve gained a solid practical understanding of how to build and train ML models including convolutional neural networks, random forests, segmentation models, and collaborative filtering models.\nCurrently I’m working on a project to identify animal species in audio recordings."
  },
  {
    "objectID": "posts/osfl-project-description/C-Results.html",
    "href": "posts/osfl-project-description/C-Results.html",
    "title": "MG ML",
    "section": "",
    "text": "This notebook collates the results of the CNN birdsong detector against different test sets and comparison with a state of the art multi-species model.\nDataset creation, cleaning, test splits, and model training are described in a different notebook called project-description which can be found in the model development walkthrough.\n\n\n\namerobin\n\n\n\n\n\n\n\ntitle\n\n\n\n\n\n\n\n\ntitle\n\n\n\n\n\nAccuracy is often sought as a metric for binary classifier evaluation, but it is only meaningful in cases where the test set contains equal numbers of positive and negative cases. Precision and Recall still work well in cases where the test set is imbalanced. The F1 score gives a way of combining precision and recall into a single metric - taking the harmonic mean of the two.\n\nAccuracy = (TP + TN) / (TP + TN + FP + FN)\nPrecision = TP / (TP + FP)\nRecall = TP / (TP + FN)\nF1 = 2 * (Precision * Recall) / (Precision + Recall)\n\nPrecision asks the question: “Out of all the positive predictions, how many were correct?”\nRecall asks the question: “Out of all the actual positives, how many did we predict correctly?”\nAccuracy is the proportion of true results (both true positives and true negatives) among the total number of examples examined."
  },
  {
    "objectID": "posts/osfl-project-description/C-Results.html#results-of-osfl-model-against-the-hawkears-osfl-output-node",
    "href": "posts/osfl-project-description/C-Results.html#results-of-osfl-model-against-the-hawkears-osfl-output-node",
    "title": "MG ML",
    "section": "Results of OSFL model against the Hawkears OSFL output node",
    "text": "Results of OSFL model against the Hawkears OSFL output node\nAt threshold which gave hawkears the highest F1 score measured against a balanced test set containting 1600 examples, the following metrics were obtained:\n\n\n\nhawkears optimal threshold"
  },
  {
    "objectID": "posts/osfl-project-description/C-Results.html#effect-of-changing-the-threshold-on-the-confusion-matrix",
    "href": "posts/osfl-project-description/C-Results.html#effect-of-changing-the-threshold-on-the-confusion-matrix",
    "title": "MG ML",
    "section": "Effect of changing the threshold on the confusion matrix",
    "text": "Effect of changing the threshold on the confusion matrix\n\nOSFL CNN Recognizer\nhttps://drive.google.com/file/d/1XaIQEqc4p0zthzxvma_nO4tW2iKYt08m/view?usp=sharing\n\n\n\n\nHawkEars OSFL output node\nhttps://drive.google.com/file/d/1TsSgGDKBxeoTWhiRSQUj1R807Y3-V9zG/view?usp=sharing"
  },
  {
    "objectID": "posts/osfl-project-description/B-project-description.html",
    "href": "posts/osfl-project-description/B-project-description.html",
    "title": "MG ML",
    "section": "",
    "text": "Process description for the Olive Sided Flycatcher recognizer.\nThis project is open sourced and available here, along with a more in depth model development walkthrough for people wanting to repeat the training process.\n\n\n\ndataset creation pipeline\n\n\nImage - dataset and model development pipeline, described below\nProject brief - Build a model to detect the Olive Sided Flycatcher in audio recordings. - The model will be used on field recordings on audio collected on ARUs in BC, Canada. - Try to make the model as good as possible, try to beat the current state of the art.\n\n\nTrain, test, inference similarity\nIn order to train a model which would be useful during production, I decided to focus on creating a high quality training set which is as similar as possible to the data the model would encounter at inference time. I took measures to avoid overfitting which can happen when the model is evaluated on examples which it was trained on, leading to over-optimistic results.\nThe model needs to work on future audio recordings as well as a backlog of historic recordings. Therefore the training set should contain audio from a variety of recording qualities and devices; if the model only encountered high quality recordings during training, it may not perform well on low quality mp3 recordings at inference time.\nI also learned that the model would need to be run on audio data collected in BC. There wasn’t a large quantity of data available in BC for training, however I had access to data collected in Canada from some nearby provinces, and others further away but which would use similar recording techniques to those used in BC. Access to field recordings from the USA would have been ideal, but I was unable to obtain these.\nTo simplify dataset creation, only the song of the Olive Sided Flycatcher was used, and not its call. The species call is less distinctive than its song. This was omitted from the positive labels unless it also occurred with the song.\nAt inference time, the model would be run on long audio recordings. A moving window is used to split the audio into segments, and the model is run on each of these segments. Since I was aiming to match the training dataset as closely as possible with the inference data, I used the same moving window technique to generate the training examples from source audio.\nI trained a model exclusively on audio collected from ARUs in the field, and built an initial dataset of training examples entirely from within the habitat of the species, with the speices song either present or absent. This ensured that the model was really learning the shape of the bird song, and not the accompanying environmental sounds or any potential artifacts introduced by layering audio samples - this was a concern I had, but not one I had read about in literature.\nInitially there was no set of labels for the absent class. By looking only at audio which contained a positive ID of the target species, I was able to infer an absent class by sampling from sections of audio before the first instance of the target species song. This was possible due to the tagging method used on WildTrax, where only the first vocalization of an individual is tagged.\nI used a model to pull out additional close range songs from previously unlabelled portions of the recordings.\nAfter that I added recordings from other locations outside of the species habitat. I paid particular attention to those model detections which were most confidently wrong, and added these to later training sets. In this way, the model was able to bootstrap examples into the training set which would contain edge cases and examples which had been most confusing (misidentified as the target species) during prior runs. This was a technique I adapted from the fastai course on deep learning, from a chapter on image classification, and have since learned has the name Hard Example Mining.\n\n\nI was given access to recordings collected across Canada by various organizations, including the start and end times of olive sided flycatcher songs within these recordings, and the unique location IDs for each recording. As a proxy for the species habitat, I chose any location ID which had at least one Olive Sided Flycatcher song detected in it."
  }
]